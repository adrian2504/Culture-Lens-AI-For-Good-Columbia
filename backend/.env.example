# LLM Provider Configuration
# Choose one: openai, anthropic, local

# OpenAI
OPENAI_API_KEY=sk-your-key-here

# Anthropic
ANTHROPIC_API_KEY=sk-ant-your-key-here

# Local LLM (Ollama)
# Install: curl -fsSL https://ollama.com/install.sh | sh
# Run: ollama run llama3.2
# No API key needed, runs on localhost:11434

# Choose provider
LLM_PROVIDER=openai
